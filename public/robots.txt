# robots.txt — last updated: 2025-08-18
# Policy: minimize crawl waste, keep private/low-value URLs out of Google,
# while allowing resources (CSS/JS/images) for proper rendering.

# Default for all crawlers
User-agent: *

# --- Admin & system (WordPress-friendly) ---
Disallow: /admin/
Disallow: /wp-admin/
Allow: /wp-admin/admin-ajax.php
Disallow: /wp-login.php
Disallow: /xmlrpc.php
Disallow: /cgi-bin/

# --- Auth & user-specific areas ---
Disallow: /login/
Disallow: /register/
Disallow: /profile/
Disallow: /user/
Disallow: /account/
Disallow: /dashboard/
Disallow: /author/ 

# --- Thank-you / order confirmation (prefer NOINDEX on-page or via header) ---
# IMPORTANT: To truly keep these out of search results, add <meta name="robots" content="noindex">
# or an X-Robots-Tag: noindex header on these URLs. Disallow alone is not sufficient.
Disallow: /thank-you/
Disallow: /checkout-complete/
Disallow: /order-received/

# --- On-site search & thin index pages ---
# Option A (budget-first): block crawling of search/facet URLs
Disallow: /search/
Disallow: /*?q=
# Option B (index-control): allow crawl but set meta robots "noindex,follow" on templates
# Choose ONE approach project-wide, don’t mix.

# --- Date/category archives (be cautious) ---
# Blanket-blocking archives can hurt internal discovery. If they are thin/duplicative, consider:
# Disallow: /archive/
# Disallow: /date/
# Alternatively, keep crawlable and use rel=canonical to key category pages or to the article.
# If you DO block, ensure there are other indexed paths that preserve topical hubs.

# --- Keep static assets crawlable (don’t block /wp-content/ etc.) ---
Allow: /wp-content/
Allow: /wp-includes/

# --- Sitemaps ---
Sitemap: https://example.com/sitemap.xml
# If you have multiple (news, images, multilingual), list each.

# --- Crawl rate notes ---
# Google ignores Crawl-delay; manage Googlebot rate in Search Console if needed.
# Bing/Yandex support Crawl-delay; only add if you're seeing server strain.
# Crawl-delay: 5
